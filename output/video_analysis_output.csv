timestamp,transcript_chunk,keywords,recommended_task
00:00:00,"Kylie Ying has worked at many interesting places, such as MIT, CERN, and Free Code Camp. She's a physicist, engineer, and basically a genius. And now she's going to teach you about machine learning in a way that is accessible to absolute beginners. What's up, you guys? So welcome to machine learning for everyone. If you are someone who is interested in machine learning and you think you are considered as everyone, then this video is for you. In this video, we'll talk about supervised and unsupervised learning models. We'll go through maybe a little bit of the logic or math behind them, and then we'll also see how we can program it on Google Collab. If there are certain things that I have done and, you know, you're somebody with more experience than me, please feel free to correct me in the comments, and we can all as a community learn from this together. So with that, let's just dive right in. With that wasting any time, let's just dive straight into the code, and I will be teaching you guys concepts as we go. So this here is the UCI machine learning repository, and basically they just have a ton of data sets that we can access. And I found this really cool one called the Magic Gamma Telescope data set. So in this data set, if you don't want to read all this information to summarize what I think it's going on, is there's this Gamma telescope, and we have all these high-energy particles hitting the telescope. Now there's a camera, there's a detector that actually records certain patterns of, you know, how this light hits the camera, and we can use properties of those patterns in order to predict what type of particle cause that radiation. So whether it was a Gamma particle, or some other like Hadron. Down here, these are all of the attributes of those patterns that we collect in the camera. So you can see that there's, you","Kylie Ying, Free Code Camp, Ying has worked, Code Camp, interesting places",Review the key concepts from this section and write a one-paragraph summary.
00:02:00,"know, some length, width, size, asymmetry, et cetera. Now we're going to use all these properties to help us discriminate the patterns and whether or not they came from a Gamma particle, or a Hadron. So in order to do this, we're going to come up here, go to the data folder, and you're going to click this Magic Zero Ford data, and we're going to download that. Now over here, I have a colab notebook open, so you go to colab.research.google.com, you start a new notebook, and I'm just going to call this the magic dataset. So actually, I'm going to call this code camp magic example. Okay, so with that, I'm going to first start with some imports. So I will import, you know, I always import numpy, I always import pandas, and I always import matplotlib, and then we'll import other things as we go. So yeah, we run that in order to run the cell, you can either click this play button here, or you can on my computer, it's a shift enter, and that that will run the cell. And here, I'm just going to let you guys know, okay, this is where I found the dataset, so I've copied and pasted this actually, but this is just where I found the dataset. And in order to import that downloaded file that we got from the computer, we're going to go over here to this folder thing, and I am literally just going to drag and drop that file into here. Okay, so in order to take a look at, you know, what does this file consist of, do we have the labels, do we not? I mean, we could open it on our computer, but we can also just do","import, order, Gamma particle, Magic, dataset",Review the key concepts from this section and write a one-paragraph summary.
00:04:00,"pandas read CSV, and we can pass in the name of this file, and let's see what it returns. So it doesn't seem like we have the labels, so let's go back to here. I'm just going to make the columns, the column labels, all of these attribute names over here, so I'm just going to take these values and make that the column names. All right, how do I do that? So basically, I will come back here, and I will create a list called calls, and I will type in all of those things with F size, F conc, and we also have F conc, one, we have F symmetry, F m, three long, F m, three trans, F alpha, let's do we have F dist and class, and class, okay, great. Now in order to label those as these columns down here in our data frame, so basically this command here just reads some CSV file that you pass in. CSV is comma about comma separated values, and turns that into a pandas data frame object. So now if I pass in a names here, then it basically assigns these labels to the columns of this data set. So I'm going to set this data frame equal to df, and then if we call the head is just like give me the first five things, now you'll see that we have labels for all of these, okay. All right, great. So one thing that you might notice is that over here the class labels we have g and h. So if","CSV, labels, data frame, data, columns",Review the key concepts from this section and write a one-paragraph summary.
00:06:00,"I actually go down here and I do data frame class dot unique, you'll see that I have either g's or h's, and these stand for gammas or hadrons. And our computer is not so good at understanding letters, right. Our computer is really good at understanding numbers. So what we're going to do is we're going to convert this to zero for g and one for h. So here I'm going to set this equal to this whether or not that equals g. And then I'm just going to say as type int. So what this should do is convert this entire column if it equals g, then this is true. So I guess that would be one. And then if it's h, it would be false with that would be zero. But I'm just converting g and h to one than zero. It doesn't really matter like if g is one and h is zero or vice versa. Let me just take a step back right now and talk about this data set. So here I have some data frame and I have all of these different values for each entry. Now this is a, you know, each of these is one sample. It's one example. It's one item in our data set. It's one data point. All of these things are kind of the same thing when I mention, oh, this is one example or this is one sample or whatever. Now each of these samples they have, you know, one quality for each or one value for each of these labels up here and then it has the class. Now what we're going to do in this specific example is try to predict for future, you know, samples, whether the class is g for gamma or h for Hadron. And that is something known as classification. Now all of these up here, these are known as our features and","class dot unique, dot unique, good at understanding, data, frame class dot",Review the key concepts from this section and write a one-paragraph summary.
00:08:00,"features are just things that we're going to pass into our model in order to help us predict the label, which in this case is the class column. So for, you know, sample zero, I have 10 different features. So I have 10 different values that I can pass into some model and I can spit out, you know, the class, the label. And I know the true label here is g. So this is, this is actually supervised learning. All right. So before I move on, let me just give you a quick little crash course on what I just said. This is machine learning for everyone. Well, the first question is what is machine learning? Well, machine learning is a subdomain of computer science that focuses on certain algorithms, which might help a computer learn from data without a programmer being there telling the computer exactly what to do. That's what we call explicit programming. So you might have heard of AI and ML and data science. What is the difference between all of these? So AI is artificial intelligence. And that's an area of computer science where the goal is to enable computers and machines to perform human-like tasks and simulate human behavior. Now, machine learning is a subset of AI that tries to solve one specific problem and make predictions using certain data. And data science is a field that attempts to find patterns and draw insights from data. And that might mean we're using machine learning. So all of these fields kind of overlap and all of them might use machine learning. So there are a few types of machine learning. The first one is supervised learning. And in supervised learning, we're using labeled inputs. So this means whatever","mugunth machine learning, learning, machine, class column, data",Review the key concepts from this section and write a one-paragraph summary.
00:10:00,"input we get, we have a corresponding output label in order to train models and to learn outputs of different new inputs that we might feed our model. So for example, I might have these pictures. Okay, to a computer, all these pictures are our pixels. They're pixels with a certain color. Now in supervised learning, all of these inputs have a label associated with them. This is the output that we might want the computer to be able to predict. So for example, over here, this picture is a cat. This picture is a dog. And this picture is a lizard. Now there's also unsupervised learning. And in unsupervised learning, we use unlabeled data to learn about patterns in the data. So here are my input data points. Again, they're just images. They're just pixels. Well, okay, let's say I have a bunch of these different pictures. And what I can do is I can feed all these to my computer. And I might not, you know, my computer is not going to be able to say, oh, this is a cat dog and lizard in terms of, you know, the output. But it might be able to cluster all these pictures. It might say, hey, all of these have something in common. All of these have something in common. And then these down here have something in common. That's finding some sort of structure in our unlabeled data. And finally, we have reinforcement learning and reinforcement learning. Well, they usually, there's an agent that is learning in some sort of interactive environment based on rewards and penalties. So let's think of a dog. We can train our dog. But there's not necessarily, you know, any wrong or right output at any given moment, right?","learning, pictures, picture, computer, output",Review the key concepts from this section and write a one-paragraph summary.
00:12:00,"Well, let's pretend that dog is a computer. Essentially, what we're doing is we're giving rewards to our computer and tell your computer, hey, this is probably something good that you want to keep doing. Well, computer agent, yeah, terminology. But in this class today, we'll be focusing on supervised learning and unsupervised learning and learning different models for each of those. All right. So let's talk about supervised learning first. So this is kind of what a machine learning model looks like. You have a bunch of inputs that are going into some model. And then the model is spitting out an output, which is our prediction. So all these inputs, this is what we call the feature vector. Now, there are different types of features that we can have. We might have qualitative features. And qualitative means categorical data. There's either a finite number of categories or groups. So one example of a qualitative feature might be gender. And in this case, there's only two here. It's for the sake of the example. I know this might be a little bit outdated. Here, we have a girl and a boy. There are two genders. There are two different categories. That's a piece of qualitative data. Another example might be, okay, we have a bunch of different nationalities. Maybe a nationality or a nation or a location. That might also be an example of categorical data. Now, in both of these, there's no inherent order. It's not like, we can rate US one and France two, Japan three, etc. There's not really any inherent order built into either of these categorical data sets. That's why we call this nominal data. Now, for nominal data, the way that we want","pretend that dog, computer, data, learning, qualitative",Review the key concepts from this section and write a one-paragraph summary.
00:14:00,"to feed it into our computer is using something called one hot encoding. So let's say that, you know, I have a data set. Some of the items in our data, some of the inputs might be from the US. So it might be from India, then Canada, then France. Now how do we get our computer to recognize that? We have to do something called one hot encoding. And basically one hot encoding is saying, okay, well, if it matches some category, make that a one. And if it doesn't, just make that a zero. So, for example, if your input were from the US, you might have one zero, zero, zero. India, you know, zero, one, zero, zero. Canada, okay, well, the item representing Canada is one. And then France, the item representing France is one. And then you can see that the rest are zeroes. That's one hot encoding. Now, there are also a different type of qualitative feature. So here on the left, there are different age groups. There's babies, toddlers, teenagers, young adults, adults, and so on, right? And on the right hand side, we might have different ratings. So maybe bad, not so good, mediocre, good, and then like great. Now, these are known as ordinal pieces of data because they have some sort of inherent order, right? Like being a toddler is a lot closer to being a baby than being an elderly person, right? Or good is closer to great than it is to really bad. So these have some sort of inherent ordering system. And so for these types of data sets, we can actually just mark them from, you know, one to five. Or we can just say, hey, for each of these, let's give it a number. And this makes sense because like,","hot encoding, hot, called one hot, France, encoding",Review the key concepts from this section and write a one-paragraph summary.
00:16:01,"for example, the thing that I just said, how good is closer to great than good is close to not good at all. Well, four is closer to five, then four is close to one. So this actually kind of makes sense. And it'll make sense for the computer as well. All right, there are also quantitative pieces of data and quantitative pieces data are numerical valued pieces of data. So this could be discrete, which means, you know, they might be integers, or it could be continuous, which means all real numbers. So for example, the length of something is a quantitative piece of data. It's a quantitative feature. The temperature of something is a quantitative feature. And then maybe how many Easter eggs I collected in my basket, this Easter egg hunt, that is an example of discrete quantitative feature. Okay, so these are continuous. And this over here is discrete. So those are the things that go into our feature vector. Those are our features that we're feeding this model, because our computers are really, really good at understanding math, right, at understanding numbers. They're not so good at understanding things that humans might be able to understand. Well, what are the types of predictions that our model can output? So in supervised learning, there are some different tasks. There's one classification. And basically classification is saying, okay, predict discrete classes. And that might mean, you know, this is a hotdog, this is a pizza, and this is ice cream. Okay, so there are three distinct classes and any other pictures of hot dogs, pizza or ice cream, I can put under these labels, hotdog pizza ice cream. This is something known as multi-class","quantitative, closer to great, quantitative feature, good, data",Review the key concepts from this section and write a one-paragraph summary.
00:18:00,"classification. But there's also binary classification. And binary classification, you might have hotdog, or not hotdog. So there's only two categories that you're working with, something that is something and something that isn't, binary classification. Okay, so yeah, other examples. So if something has positive or negative sentiment, that's binary classification. Maybe you're predicting your pictures if they're cats or dogs, that's binary classification. Maybe, you know, you are writing an email filter and you're trying to figure out if an email is spam or not spam. So that's also binary classification. Now, for multi-class classification, you might have, you know, cat dog, lizard, dolphin, shark, rabbit, etc. We might have different types of fruit to like orange, apple, pear, etc. And then maybe different plant species. But multi-class classification just means more than two. Okay, and binary means we're predicting between two things. There's also something called regression when we talk about supervised learning. And this just means we're trying to predict continuous values. So instead of just trying to predict different categories, we're trying to come up with a number that, you know, is on some sort of scale. So some examples. So some examples might be the price of Ethereum tomorrow. Or it might be, okay, what is going to be the temperature? Or it might be what is the price of this house, right? So these things don't really fit into discrete classes. We're trying to predict a number that's as close to the true value as possible using different features of our data set. So that's exactly what our model looks like in supervised learning. Now, let's talk about the model itself. How do we make this model learn? Or","binary classification, classification, binary, predict, hotdog",Train a classifier (like a Decision Tree or SVM) on the Iris dataset and evaluate its accuracy.
00:20:00,"how can we tell whether or not it's even learning? So before we talk about the models, let's talk about how can we actually like evaluate these models or how can we tell whether something is a good model or bad model. So let's take a look at this data set. So this data set has this is from a diabetes, a Pima Indian diabetes data set. And here we have different number of pregnancies, different glucose levels, blood pressure, skin thickness, insulin, BMI, age, and then the outcome, whether or not they have diabetes. One for they do, zero for they don't. So here, all of these are quantitative features, right? Because they're all on some scale. So each row is a different sample in the data. So it's a different example. It's one person's data. And each row represents one person in this data set. Now this column, each column represents a different feature. So this one here is some measure of blood pressure levels. And this one over here as we mentioned is the output label. So this one is whether or not they have diabetes. And as I mentioned, this is what we would call a feature vector because these are all of our features in one sample. And this is what's known as the target or the output for that feature vector. That's what we're trying to predict. And all of these together is our features matrix X. And over here, this is our labels or targets vector Y. So I've condensed this to a chocolate bar to kind of talk about some of the other concepts in machine learning. So over here, we have our X,","data set, data, Pima Indian diabetes, set, Pima Indian",Review the key concepts from this section and write a one-paragraph summary.
00:22:00,"our features matrix. And over here, this is our label Y. So each row of this will be fed into our model, right? And our model will make some sort of prediction. And what we do is we compare that prediction to the actual value of Y that we have in our label data set because that's the whole point of supervised learning is we can compare what our model is outputting to what is the truth actually. And then we can go back and we can adjust some things so that the next iteration we get closer to what the truth value is. So that whole process here, the tinkering that, okay, what's the difference? Where did we go wrong? That's what's chunk right here. Do we want to really put our entire chocolate bar into the model to train our model? Not really, right? Because if we did that, then how do we know that our model can do well on new data that we haven't seen? Like if I were to create a model to predict whether or not someone has diabetes, let's say that I just train on my data and I see that on my training data it does well. I go to some hospital and I'm like, here's my model. I think you can use this to predict if somebody has diabetes. Do we think that would be effective or not? Probably not, right? Because we haven't assessed how well our model can generalize. Okay, it might do well after our model has seen this data over and over and over again. But what about new data? Can our model handle new data? Well, how do we get our model to assess that? So we actually break up our whole data set that we have","model, features matrix, data, matrix, features",Review the key concepts from this section and write a one-paragraph summary.
00:24:00,"into three different types of data sets. We call it the training data set, the validation data set, and the testing data set. And you know, you might have 60% here, 20% and 20% or 80, 10, and 10. It really depends on how many statistics you have. I think either of those would be acceptable. So what we do is then we feed the training data set into our model. We come up with, you know, this might be a vector of predictions corresponding with each sample that we put into our model. We figure out, okay, what's the difference between our prediction and the true values? This is something known as loss. Loss is, you know, what's the difference here in some numerical quantity, of course. And then we make adjustments. And that's what we call training. Okay. So then once, you know, we've made a bunch of adjustments, we can put our validation set through this model. And the validation set is kind of used as a reality check during or after training to ensure that the model can handle unseen data still. So every single time after we train one iteration, we might stick the validation set in and see, hey, what's the loss there? And then after our trainings over, we can assess the validation set and ask, hey, what's the loss there? But one key difference here is that we don't have that training step. This loss never gets fed back into the model, right? That feedback loop is not closed. All right. So let's talk about loss really quickly. So here I have four different types of models. I have some sort of data that's being fed into the model and then some output. Okay. So this output here is pretty far from, you know, this truth that we want. And so this loss","data, set, data set, loss, validation set",Review the key concepts from this section and write a one-paragraph summary.
00:26:00,"is going to be high in model B. Again, this is pretty far from what we want. So this loss is also going to be high. Let's give it 1.5. Now this one here, it's pretty close. I mean, maybe not almost, but pretty close to this one. So that might have a loss of 0.5. And then this one here is maybe further than this, but still better than these two. So that loss might be 0.9. Okay. So which of these models performs the best? Well, Model C has the smallest loss. So it's probably Model C. Okay. Now let's take Model C after, you know, we've come with all these models and we've seen, okay, Model C is probably the best model. We take Model C and we run our test set through this model. And this test set is used as a final check to see how generalizable that chosen model is. So if I, you know, finished training my diabetes data set, then I could run it through some chunk of the data and I can say, oh, like, this is how we perform on data that it's never seen before at any point during the training process. Okay. And that loss, that's the final reported performance of my test set or this would be the final reported performance of my model. Okay. So let's talk about this thing called loss because I think I kind of just lost over it, right? So loss is the difference between your prediction and the actual label. So this would give a slightly higher loss than this. And this would even give a higher loss because it's even more off in computer science. We like formulas, right? We like formulaic ways of describing things. So here","model, loss, set, pretty, models",Review the key concepts from this section and write a one-paragraph summary.
00:28:00,"are some examples of loss functions and how we can actually come up with numbers. This here is known as L1 loss. And basically L1 loss just takes the absolute value of whatever your, you know, real value is whatever the real output label is, subtracts the predicted value and takes the absolute value of that. Okay. So the absolute value is a function that looks something like this. So the further off you are, the greater your loss is, right? In either direction. So if your real value is off from your predicted value by 10, then your loss for that point would be 10. And then this sum here just means hey, we're taking all the points in our data set and we're trying to figure out the sum of how far everything is. Now we also have something called L2 loss. So this loss function is quadratic, which means that if it's close, the penalty is very minimal. And if it's off by a lot, then the penalty is much, much higher. Okay. And this instead of the absolute value, we just square the difference between the two. Now there's also something called binary cross entropy loss. It looks something like this. And this is for binary classification. This might be the loss that we use. So this loss, you know, I'm not going to really go through it too much, but you just need to know that loss decreases as the performance gets better. So there are some other measures of performance as well. So for example, accuracy. What is accuracy? So let's say that these are pictures that I'm feeding my model. Okay.","loss, absolute, real, numbers, function",Review the key concepts from this section and write a one-paragraph summary.
00:30:01,"And these predictions might be apple, orange, orange, apple. Okay, but the actual is apple, orange, apple, apple. So three of them were correct and one of them was incorrect. So the accuracy of this model is three quarters or 75%. All right, coming back to our collab notebook, I'm going to close this a little bit. Again, we've imported stuff up here. And we've already created our data frame right here. And this is all of our data. This is what we're going to use to train our models. So down here, again, if we now take a look at our data set, you'll see that our classes are now zeros and ones. So now this is all numerical, which is good because our computer can now understand that. Okay. And you know, it would probably be a good idea to maybe kind of plot, hey, dude, these things have anything to do with the class. So here, I'm going to go through all of the labels. So for label in the columns of the state of frame, so this just gets me the list. Actually, we have the list, right, it's called. So let's just use that. You might be less confusing of everything up till the last thing, which is the class. So I'm going to take all these 10 different features. And I'm going to plot them as a histogram. So and now I'm going to plot them as a histogram. So basically, if I take that data frame and I say, okay, for everything where the class is equal to one. So these are all of our gammas. Remember, now for that portion of the data frame, if I look at this label. So now these, okay, what this part here is saying is inside","apple, orange, data, data frame, frame",Review the key concepts from this section and write a one-paragraph summary.
00:32:01,"the data frame, get me everything where the class is equal to one. So that's all of these would fit into that category, right? And now let's just look at the label column. So the first label would be F length, which would be this column. So this command here is getting me all the different values that belong to class one for this specific label. And that's exactly what I'm going to put into the histogram. And now I'm just going to tell you know, map plot lib, make the color blue, make label this as, you know, gamma, set alpha. Why do I keep doing that? Alpha equal to 0.7. So that's just like the transparency. And then I'm going to set density equal to true. So that when we compare it to the hadrons here, we'll have a baseline for comparing them. Okay, so the density being true just basically normalizes these distributions. So, you know, if you have 200 and of one type and then 50 of another type, well, if you drew the histograms, it would be hard to compare because one of them would be a lot bigger than the other, right? But by normalizing them, we kind of are distributing them over how many samples there are. All right. And then I'm just going to put a title on here, make that the label, the y label. So because it's density, the y label is probability. And the x label is just going to be the label. What is going on? And I'm going to include a legend and plt.show just means, okay, display the plot. So if I run that, just be up to the last item. So we want a list, right? Not just the last item. And","label, data frame, equal, make, density",Review the key concepts from this section and write a one-paragraph summary.
00:34:00,"now we can see that we're plotting all of these. So here we have the length. Oh, and I made this gamma. So this should be hadron. Okay. So the gamma's in blue, the hadrons aren't red. So here we can already see that, you know, maybe if the length is smaller, it's probably more likely to be gamma, right? And we can kind of, you know, these all look somewhat similar. But here, okay, clearly if there's more asymmetry, or if, you know, this asymmetry measure is larger than it's probably hadron. Okay. Oh, this one's a good one. So F alpha seems like hadrons are pretty evenly distributed, whereas if this is smaller, it looks like there's more gammas in that area. Okay. So this is kind of what the data that we're working with, we can kind of see what's going on. Okay. So the next thing that we're going to do here is we are going to create our train, our validation, and our test datasets. I'm going to set train, valid, and test to be equal to this, so numpy.split, I'm just splitting up the data frame. And if I do this sample where I'm sampling everything, this will basically shuffle my data. Now, if I want to pass in where exactly I'm splitting my dataset, so the first split is going to be maybe at 60%. So I'm just going to say 0.6 times the length of this data frame. So, and then cast that 10 integer. That's going to be the first place where, you know, I cut it off, and that will be my training data. Now, if I then go to 0.8, this basically means","data, gamma, length, kind, plotting",Review the key concepts from this section and write a one-paragraph summary.
00:36:00,"everything between 60% and 80% of the length of the dataset will go towards validation, and then like everything from 80 to 100 is going to be my test data. So I can run that. And now, if we go up here and we inspect this data, we'll see that these columns seem to have values in like the 100s, whereas this one is 0.03. Right? So the scale of all these numbers is way off. And sometimes that will affect our results. So one thing that we would want to do is scale these so that they are, you know, so that it's now relative to maybe the mean and the standard deviation of that specific column. I'm going to create a function called scale dataset, and I'm going to pass in the data frame. And that's what I'll do for now. Okay, so the x values are going to be, you know, I take the data frame and let's assume that the columns are going to be, you know, that the label will always be the last thing in the data frame. So what I can do is say data frame dot columns all the way up to the last item and get those values. Now, for my y, well, it's the last column. So I can just do this. I can just index into that last column and then get those values. Now in, so I'm actually going to import something known as the standard scalar from sk learn. So if I come up here, I can go to sk learn dot pre processing. And I'm going to import standard scalar.","data frame, data, test data, frame, columns",Review the key concepts from this section and write a one-paragraph summary.
00:38:00,"I have to run that cell. I'm going to come back down here. And now I'm going to create a scalar and use that scalar. So standard scalar. And with the scalar, what I can do is actually just fit and transform x. So here, I can say x is equal to scalar dot fit, fit transform x. So what that's doing is saying, okay, take x and fit the standard scalar to x. And then transform all those values and what would it be? And that's going to be our new x. All right. And then I'm also going to just create, you know, the whole data as one huge 2d numpy array. And in order to do that, I'm going to call h stack. So h stack is saying, okay, take an array and another array and horizontally stack them together. That's what the h stands for. So by horizontally stacked them together, just like put them side by side, okay, not on top of each other. So what am I stacking? Well, I have to pass in something so that it can stack x and y. And now, okay, so numpy is very particular about dimensions, right? So in this specific case, our x is a two-dimensional object, but y is only a one-dimensional thing. It's only a vector of values. So in order to now reshape it into a 2d item, we have to call numpy dot reshape. And we can pass in the dimensions of its reshape. So if I pass a negative 1 comma 1, that just means, okay, make this a 2d array, where the negative 1 just means infer what this dimension value would be, which ends up being the length of y. This would be the same as literally doing this, but the negative 1 is easier","run that cell, scalar, fit, array, stack",Review the key concepts from this section and write a one-paragraph summary.
00:40:00,"because we're making a computer do the hard work. So if I stack that, I'm going to then return the data x and y. Okay, so one more thing is that if we go into our training data set, okay, again, this is our training data set, and we get the length of the training data set, but where the training data sets class is 1. So remember that this is the gammas. And then if we print that, and we do the same thing but 0, we'll see that, you know, there's around 7,000 of the gammas, but only around 4,000 of the hadrons. So that might actually become an issue. And instead, what we want to do is we want to over sample our training data set. So that means that we want to increase the number of these values so that these kind of match better. And surprise, surprise, there is something that we can import that will help us do that. So I'm going to go do from inblurn.oversampling. And I'm going to import this random over sampler. Run that cell and come back down here. So I will actually add in this parameter called over sample and set that to false for default. And if I do want to over sample, then what I'm going to do and by over sample, so if I do want to over sample, then I'm going to create this ROS and set it equal to this random over sampler. And then for x and y, I'm","training data set, training data, data set, data, hard work",Review the key concepts from this section and write a one-paragraph summary.
00:42:00,"just going to say, okay, just fit and re sample x and y. And what that's doing is saying, okay, take more of the less class. So take the less class and keep sampling from there to increase the size of our data set of that smaller class so that they now match. So if I do this and I scale data set and I pass in the training data set where over sample is true. So this, let's say this is train and then x train y train. Oops, what's going on? Oh, these should be columns. So basically, what I'm doing now is I'm just saying, okay, what is the length of y train? Okay, now it's 14,800, whatever. And now let's take a look at how many of these are type one. So actually, we can just sum that up. And then we'll also see that if we instead switch the label and ask how many of them are the other type, it's the same value. So now these have been evenly rebalanced. Okay, well, okay, so here I'm just going to make this the validation data set. And then the next one, I'm going to make this the test data set. All right, and we're actually going to switch over sample here to false. Now, the reason why I'm switching that to false is because my validation and my test sets are for the purpose of, you know, if I have data that I haven't seen yet, how does my sample perform on those? And I don't want to over sample for that right now. Like I don't","data set, data, set, sample, class",Review the key concepts from this section and write a one-paragraph summary.
00:44:00,"care about balancing those. I want to know if I have a random set of data that's unlabeled. Can I trust my model? Right. So that's why I'm not over sampling. I run that and again, what is going on? Oh, it's because we already have this train. So I have to go come up here and split that data frame again. And now let's run these. Okay. So now we have our data properly formatted. And we're going to move on to different models now. And I'm going to tell you guys a little bit about each of these models. And then I'm going to show you how we can do that in our code. So the first model that we're going to learn about is K&N or K nearest neighbors. Okay. So here I've already drawn a plot on the y-axis. I have the number of kids that a family might have. And then on the x-axis, I have their income in terms of thousands per year. So if someone's making 40,000 a year, that's where this would be. And if somebody making 320, that's where that would be. Somebody has zero kids. It'd be somewhere along this axis. Somebody has five. It'd be somewhere over here. Okay. And now I have these plus signs and these minus signs on here. So what I'm going to represent here is the plus sign means that they own a car. And the minus sign is going to represent no car. Okay. So your initial thought should be, okay, I think this is binary classification because all of our points, all of our samples have labels. So this is a sample with","care about balancing, care, data, balancing, random set",Review the key concepts from this section and write a one-paragraph summary.
00:46:00,"the plus label. And this here is another sample with the minus label. This is an abbreviation for width that I'll use. All right. So we have this entire data set and maybe around half the people own a car. And maybe around half the people don't own a car. Okay. Well, what if I had some new point? Let me use choose a different color. I'll use this nice green. Well, what if I have a new point over here? So let's say that somebody makes 40,000 a year and has two kids. What do we think that would be? Well, just logically looking at this plot, you might think, okay, it seems like they wouldn't have a car, right? Because that kind of matches the pattern of everybody else around them. So that's a whole concept of this nearest neighbors is you look at, okay, what's around you? And then you're basically like, okay, I'm going to take the label of the majority that's around me. So the first thing we have to do is we have to define a distance function. And a lot of times in, you know, 2D plots like this, our distance function is something known as Euclidean distance. And Euclidean distance is basically just this straight line distance like this. Okay. So this would be the Euclidean distance. It seems like there's this point, there's this point, there's that point, etc. So the length of this line, this green line that I just drew, that is","Euclidean distance, distance, point, label, Euclidean",Review the key concepts from this section and write a one-paragraph summary.
00:48:00,"what's known as Euclidean distance. If we want to get technical with that, this exact formula is the distance, here let me zoom in. The distance is equal to the square root of 1.x minus the other points x squared plus extend that square root the same thing for y. So y1 of 1 minus y2 of the other squared. Okay. So we're basically trying to find the length, the distances, the difference between x and y. And then square each of those, sum it up and take the square root. Okay. So I'm going to erase this so it doesn't clutter my drawing. But anyways, now going back to this plot. So here in the nearest neighbor algorithm, we see that there is a k, right. And this k is basically telling us, okay, how many neighbors do we use in order to judge what the label is? So usually we use a k of maybe, you know, three or five, depends on how big our data set is. But here I would say maybe a logical number would be three or five. So let's say that we take k to be equal to three. Okay. Well, of this data point that I drew over here, let me use green to highlight this. Okay. So of this data point that I drew over here, it looks like the three closest points are definitely this one, this one. And then this one has a length of four. And this one seems like it'd be a little bit further than four. So actually this would be our, these would be our three points. Well, all those points are blue. So chances","Euclidean distance, Euclidean, square root, square, distance",Review the key concepts from this section and write a one-paragraph summary.
00:50:00,"are my prediction for this point is going to be blue. It's going to be probably don't have a car. All right. Now what if my point is somewhere, what if my point is somewhere over here? Let's say that a couple has four kids and they make 240,000 a year. All right. Well, now my closest points are this one. Probably a little bit over that one. And then this one. Right. Okay. Still all pluses. Well, this one is more than likely to be plus. Right. Now let me get rid of some of these just so that it looks a little bit more clear. All right. Let's go through one more. What about a point that might be right here. Okay. Let's see. Well, definitely this is the closest. Right. This one is also closest. And then it's really close between the two of these. But if we actually do the mathematics, it seems like if we zoom in, this one is right here. And this one is in between these two. So this one here is actually shorter than this one. And that means that that top one is the one that we're going to take. Now what is the majority of the points that are closed by? Well, we have one plus here. We have one plus here. And we have one minus here, which means that the pluses are the majority. And that means that this label is probably somebody with a car. Okay.","point, blue, closest, points, prediction",Review the key concepts from this section and write a one-paragraph summary.
00:52:01,"So this is how K nearest neighbors would work. It's that simple. And this can be extrapolated to further dimensions, to higher dimensions. You know, if you have here, we have two different features. We have the income. And then we have the number of kids. But let's say we have 10 different features, we can expand our distance function so that it includes all 10 of those dimensions. We take the square root of everything. And then we figure out which one is the closest to the point that we desire to classify. Okay. So that's K nearest neighbors. So now we've learned about K nearest neighbors. Let's see how we would be able to do that within our code. So here I'm going to label the section K nearest neighbors. And we're actually going to use a package from SK learned. So the reason why we use these packages is so that we don't have to manually code all of these things ourselves because it would be really difficult and chances are the way that we would code it either we'd have bugs or it'd be really slow or I don't know a whole bunch of issues. So what we're going to do is hand it off to the pros. From here I can say, okay, from SK learn, which is this package dot neighbors. I'm going to import K neighbors classifier because we're classifying. Okay. So I've run that. And our K and N model is going to be this K neighbors classifier. And we can pass in a parameter of how many neighbors we want to use. So first let's see what happens if we just use one. So now if I do K and N model dot fit, I can pass in my x training set and my y train data. Okay. So that effectively fits this model.","nearest neighbors, neighbors, nearest, dimensions, neighbors classifier",Review the key concepts from this section and write a one-paragraph summary.
00:54:00,"And let's get all the predictions. So Y, K and N, or I guess yeah let's do Y predictions. And my Y predictions are going to be K and N model dot predict. So let's use the test set x test. Okay. All right. So if I call my predict, you'll see that we have those. But if I get my truth values for that test set, you'll see that this is what we actually do. So just again, this we got five out of six of them. Okay. Great. So let's actually take a look at something called the classification report that's offered by SK learned. So if I go to from SK learn dot metrics, import classification report, what I can actually do is say, hey print out this classification report for me. And let's check, you know, I'm giving you the Y test and the Y prediction. We run this and we see we get this whole entire chart. So I'm going to tell you guys a few things on this chart. All right. This accuracy is 82%, which is actually pretty good. That's just saying, hey, if we just look at, you know, what each of these new points, what it's closest to, then we actually get an 82% accuracy, which means how many do we get right versus how many total are there? Now precision is saying, okay, you might see that we have it for class one or class zero and class one. What precision is saying, let's go to this Wikipedia diagram over here because I actually kind of like this diagram. So here, this is our entire data set. And on the left over here, we have everything that we know is positive. So everything that is actually truly positive, that we've labeled positive in our original data set. And over here, this is everything that's truly negative. Now in this circle, we have things that are","predictions, test, set, classification report, classification",Train a classifier (like a Decision Tree or SVM) on the Iris dataset and evaluate its accuracy.
00:56:00,"positive that were labeled positive by our model. On the left here, we have things that are truly positive because, you know, this side is the positive side and the side is the negative side. So these are truly positive. Whereas all these ones out here, well, they should have been positive, but they were labeled as negative. And in here, these are the ones that we've labeled positive, but they're actually negative. And out here, these are truly negative. So precision is saying, okay, out of all the ones we've labeled as positive, how many of them are true positives. And recall is saying, okay, out of all the ones that we know are truly positive, how many do we actually get right? Okay. So going back to this over here, our precision score. So again, precision out of all the ones that we've labeled as the specific class, how many of them are actually that class? It's 77 and 84%. Now recall how out of all the ones that are actually this class, how many of those do we get? This is 68% and 89%. All right. So not too shabby. We clearly see that this recall and precision for like the class zero is worse than class one. Right. So that means for Hadron, it's worked for Hadrons and for our Gamers. This F1 score over here is kind of a combination of the precision and recall score. So we're actually going to mostly look at this one because we have an unbalanced test data set. So here we have a measure of 72 and 87, or 0.72 and 0.87, which is not too shabby. All right. Well, what if we made this three? So we actually see that, okay, so what was it originally with one?","positive, labeled, side, precision, class",Review the key concepts from this section and write a one-paragraph summary.
00:58:00,"We see that our F1 score, it was 0.72 and then 0.87 and then our accuracy was 82%. So if I change that to three, all right. So we've kind of increased zero at the cost of one and then our overall accuracy is 81. So let's actually just make this five. All right. So, you know, again, very similar numbers. We have 82% accuracy, which is pretty decent for a model that's relatively simple. Okay. The next type of model that we're going to talk about is something known as naive bays. Now in order to understand the concepts behind naive bays, we have to be able to understand conditional probability and bays rule. So let's say I have some sort of data set that's shown in this table right here. People who have COVID are over here in this red row and people who do not have COVID are down here in this green row. Now what about the COVID test? Well, people who have tested positive are over here in this column and people who have tested negative are over here in this column. Okay. Yeah. So basically our categories are people who have COVID and test positive, people who don't have COVID but test positive. So it falls positive. People who have COVID and test negative, which is a false negative and people who don't have COVID and test negative, which good means you don't have COVID. Okay. So let's make this slightly more legible. And here in the margins, I've written down the sums of whatever it's referring to. So this here is the sum of this entire row. And this here might be the sum","COVID, People, test, COVID and test, positive",Review the key concepts from this section and write a one-paragraph summary.
01:00:00,"of this column over here. Okay. So the first question that I have is what is the probability of having COVID given that you have a positive test? And in probability, we write that out like this. So the probability of COVID given, so this line, that vertical line means given that, you know, some condition. So given a positive test. Okay. So what is the probability of having COVID given a positive test? So what this is asking is saying, okay, let's go into this condition. So the condition of having a positive test, that is this slice of the data, right? That means if you're in this slice of data, you have a positive test. So given that, we have a positive test given in this condition, in this circumstance, we have a positive test. So what's the probability that we have COVID? Well, if we're just using this data, the number of people that have COVID is 531. So I'm going to say that there's 531 people that have COVID. And then now we divide that by the total number of people that have a positive test, which is 551. Okay. So that's the probability and doing a quick division, we get that this is equal to around 96.4%. So according to this data set, which is data that I made up off the top of my head. So it's not actually real COVID data. But according to this data, the probability of having COVID given that you test positive is 96.4%. All right. Now with that, let's talk about","positive test, COVID, positive, test, probability",Review the key concepts from this section and write a one-paragraph summary.
01:02:00,"Bayes rule, which is this section here. Let's ignore this bottom part for now. So Bayes rule is asking, okay, what is the probability of some event A happening? Given that B happened. So this, we already know has happened. This is our condition, right? Well, what if we don't have data for that, right? Like, what if we don't know what the probability of A given B is? Well, Bayes rule is saying, okay, well, you can actually go and calculate it as long as you have the probability of B given A, the probability of A and the probability of B. Okay. And this is just a mathematical formula for that. All right. So here we have Bayes rule. And let's actually see Bayes rule in action. Let's use it on an example. So here, let's say that we have some disease statistics. Okay. So not COVID different disease. And we know that the probability of obtaining a false positive is 0.05, probability of obtaining a false negative is 0.01, and the probability of the disease is 0.1. Okay. What is the probability of the disease given that we got a positive test? How do we even go about solving this? So what do I mean by false positive? What's a different way to rewrite that? A false positive is when you test positive, but you don't actually have the disease. So this here is a probability that you have a positive test given no disease. Right? And similarly, for the false negative, it's a probability that you test negative given that you actually have the disease. So if I put that into a chart, for example, and this might be my positive and negative tests, and this might be my diseases disease","probability, Bayes rule, Bayes, disease, positive",Review the key concepts from this section and write a one-paragraph summary.
01:04:00,"and no disease. Well, the probability that I test positive but actually have no disease. Okay. That's 0.05 over here. And then the false negative is up here for 0.01. So I'm testing negative, but I don't actually have the disease. This, so the probability that you test positive and you don't have the disease, plus the probability that you test negative given that you don't have the disease, that should sum up to 1. Okay. Because if you don't have the disease, then you should have some probability that you're testing positive and some probability that you're testing negative, but that probability in total should be 1. So that means that the probability of negative and no disease, this should be the reciprocal, this should be the opposite. So it should be 0.95 because it's 1 minus whatever this probability is. And then similarly, oops. Up here, this should be 0.99 because the probability that we, you know, test negative and have the disease, plus the probability that we test positive and have the disease should equal 1. So this is our probability chart. And now this probability of disease being 0.1 just means I have 10% probability of actually having the disease, right? Like in the general population, the probability that I have the disease is 0.1. Okay. So what is the probability that I have the disease given that I got a positive, I got a positive test. Well, remember that we can write this out in terms of phase rule, right? So if I use this rule up here, this is the probability of a positive test given that I have the disease times the probability of the disease, divided by the probability of the evidence, which is my positive test. All","probability, disease, positive, test, negative",Review the key concepts from this section and write a one-paragraph summary.
01:06:00,"right. Now let's plug in some numbers for that. The probability of having a positive test given that I have the disease is 0.99. And then the probability that I have the disease is this value over here, 0.1. Okay. And then the probability that I have a positive test at all should be, okay, what is the probability that I have a positive test given that I actually have the disease? And then having the having the disease. And then the other case where the probability of me having a negative test given, or sorry, positive test giving no disease times the probability of not actually having a disease. Okay. So I can expand that probability of having a positive test out into these two different cases. I have a disease and then I don't. And then what's the probability of having positive test in either one of those cases? So that expression would become 0.99 times 0.1 plus 0.05. So that's the probability that I'm testing positive but don't have the disease. And the times probably that I don't actually have the disease. So that's 1 minus 0.1. The probability that the population that I have the disease is 90 percent. So 0.9. And let's do that multiplication. And I get an answer of 0.6875 or 68.75 percent. Okay. All right. So we can actually expand that. We can expand Bayes rule and apply it to classification. And this is what we call naive Bayes. So first a little terminology. So the posterior","probability, disease, positive test, positive, test",Review the key concepts from this section and write a one-paragraph summary.
01:08:00,"is this over here because it's asking, hey, what is the probability of some class ck? So by ck I just mean you know the different categories. So c for category or class or whatever. So category one might be cats. Category two, dogs. Category three, lizards. All the way we have k categories, k is just some number. Okay. So what is the probability of having of this specific sample x? So this is our feature vector of this one sample. What is the probability of x fitting into category one, two, three, four, whatever? Right. So that's what this is asking. What is the probability that you know it's actually from this class given all this evidence that we see the x's? So the likelihood is this quantity over here saying, okay, well given that, you know, assume, assume we are, assume that this class is class ck. Okay. Assume that this is a category. Well, what is the likelihood of actually seeing x all these different features from that category? And then this here is the prior. So like in the entire population of things, what are the probabilities? What is the probability of this class in general? Like if I have you know in my entire data set, what is the percentage? What is the chance that this image is a cat? How many cats do I have? Right. And then this down here is called the evidence because what we're trying to do is we're changing our prior. We're creating this new posterer probability built upon the prior by using some sort of evidence. Right. And that evidence is a probability of x. So that's some vocab. And","probability, category, class, assume, evidence",Review the key concepts from this section and write a one-paragraph summary.
01:10:00,"this here is a rule for naive base. Whoa. Okay. Let's say just that a little bit. Okay. So what is, let me use a different color. What is this side of the equation asking? It's asking what is the probability that we are in some class k ck given that, you know, this is my first input. This is my second input. This is, you know, my third fourth. This is my end input. So let's say that our classification is, do we play soccer today or not? Okay. And let's say our x's are, okay, is it how much wind is there? How much rain is there? And what day of the week is it? So let's say that it's raining. It's not windy, but it's Wednesday. Do we play soccer or do we not? So let's use base rule on this. So this here is equal to the probability of x1, x2, all these joint probabilities given class k times the probability of that class all over the probability of this evidence. Okay. So what is this fancy symbol over here? This means proportional to, so how our equal sign means it's equal to, this like little squiggly sign means that this is proportional to. Okay. And this denominator over here, you might notice that it has no impact on the class, like this, that number doesn't depend on the class, right? So this is going to be constant for all of our different classes. So what I'm going to do is make things simpler. So I'm just going to say that","class, probability, naive base, input, equal",Review the key concepts from this section and write a one-paragraph summary.
01:12:00,"this probability, x1, x2 all the way to xn, this is going to be proportional to the numerator. I don't care about the denominator because it's the same for every single class. So this is proportional to x1, x2, xn given class k times the probability of that class. Okay. All right. So in naive bays, the point of it being naive is that we're actually, this joint probability, we're just assuming that all of these different things are all independent. So in my soccer example, you know, the probability that we're playing soccer, or the probability that, you know, it's windy and it's rainy and it's Wednesday, all these things are independent. We're assuming that they're independent. So that means that I can actually write this part of the equation here as this. So each term in here, I can just multiply all them together. So the probability of the first feature given that it's class k times the probability of second feature given that's like class k all the way up until, you know, the end feature of given that it's class k. So this expands to all of this. All right. Which means that this here is now proportional to the thing that we just expanded times this. So I'm going to write that out. So the probability of that class, and I'm actually going to use this symbol. So what this means is it's a huge multiplication. It means multiply everything to the right of this. So this probability,","probability, class, proportional, independent, times",Review the key concepts from this section and write a one-paragraph summary.
01:14:01,"x given some class k, but do it for all the i. So i, what is i? Okay. We're going to go from the first x i all the way to the end. So that means for every single i, we're just multiplying these probabilities together. And that's where this up here comes from. So to wrap this up, oops, this should be a line. To wrap this up in plain English, basically what this is saying is a probability that, you know, were in some category given that we have all these different features is proportional to the probability of that class in general times the probability of each of those features given that were in this one class that we're testing. So the probability of it, you know, of us playing soccer today, given that it's rainy, not windy, and it's Wednesday is proportional to, okay, well, what is what is the probability that we play soccer anyways? And then times the probability that it's rainy, given that we're playing soccer, times the probability that it's not windy, given that we're playing soccer. So how many times are we playing soccer when it's windy? You know, and then how many times are what's the probability that's Wednesday given that we're playing soccer? Okay. So how do we use this in order to make a classification? So that's where this comes in. Our Y hat, our predicted Y, is going to be equal to something called the arg max. And then this expression over here because we want to take the arg max, well, we want. So, okay, if I write out this, again, this means the probability of being in some class ck, given all of our evidence,","probability, playing soccer, soccer, playing, times",Review the key concepts from this section and write a one-paragraph summary.
01:16:01,"well, we're going to take the k that maximizes this expression on the right. That's what arg max means. So if k is in 0, oops, 1 through k, so this is how many categories there are, we're going to go through each k and we're going to solve this expression over here and find the k that makes that the largest. Okay. And remember that instead of writing this, we have now a formula thanks to base rule for helping us approximate that, right? And something that maybe we can, maybe we have like the evidence for that, we have the answers for that based on our training set. So this principle of going through each of these and finding whatever class, whatever category maximizes this expression on the right, this is something known as MAP for short or maximum, a posteriori, pick the hypothesis, so pick the k that is the most probable so that we minimize the probability of misclassification, right? So that is MAP, that is naive base, back to the notebook. So just like how I imported k neighbors classifier up here, for naive base, I can go to sklearn.naivbase and I can import Gaussian naive base, right? And here I'm going to send my naive base model is equal, this is very similar to what we had above. And I'm just going to say with this model, we","naive base, maximizes this expression, base, expression, naive",Review the key concepts from this section and write a one-paragraph summary.
01:18:01,"are going to fit x-train and y-train, right? Just like above. So this I might actually, so I'm going to set that. And exactly, just like above, I'm going to make my prediction. So here I'm going to instead use my naive base model. And of course, I'm going to run the classification report again. So I'm actually just going to put these in the same cell. But here, we have the new y prediction and then y test is still our original test data set. So if I run this, you'll see that. Okay, what's going on here? We get worst scores, right? Our precision for all of them, they look slightly worse. And our, you know, for our precision, our recall, our f1 score, they look slightly worse for all the different categories. And our total accuracy, I mean, it's still 72%, which is not too shabby, but it's still 72%, okay? Which, you know, is not that great. Okay, so let's move on to logistic regression. Here I've drawn a plot. I have y. So this is my label on one axis. And then this is maybe one of my features. So let's just say I only have one feature in this case, text zero, right? Well, we see that, you know, I have a few of one class type down here. And we know it's one class type because it's zero. And then we have our other class type one up here. Okay. So many of you guys are familiar with regression. So let's start there. If I were to draw a regression","x-train and y-train, fit x-train, class type, y-train, class",Review the key concepts from this section and write a one-paragraph summary.
01:20:00,"line through this, it might look something like, like this. Right? Well, this doesn't seem to be a very good model. Like why would we use this specific line to predict why? Right? It's, it's iffy, okay? For example, we might say, okay, well, it seems like, you know, everything from here down words would be one class type. And here upwards would be another class type. But when you look at this, you're just, you, you visually can tell, okay, like that line doesn't make sense. Things are not, those dots are not along that line. And the reason is because we are doing classification, not regression. Okay. Well, first of all, let's start here. We know that this model, if we just use this line, it equals m x. So whatever this, let's just say it's x plus b, which is the y intercept, right? And m is the slope. But when we use a linear regression, is it actually y hat? No, it's not, right? So when we're working with linear regression, what we're actually estimating in our model is a probability. What's a probability between zero and one that is class zero or class one? So here, let's rewrite this as p equals m x plus b. Okay. Well, m x plus b, that can range, you know, from negative infinity to infinity, right? For any, for any value of x, it goes from negative infinity to infinity. But probability, we know probably, one of the rules of probability is that probability has to stay between zero and one. So how do we fix this? Well, maybe instead of just setting the probability equal to that, we can set the odds equal","probability, line, class, infinity, regression",Review the key concepts from this section and write a one-paragraph summary.
01:22:00,"to this. So by that, I mean, okay, let's do probability divided by one minus the probability. Okay. So now it becomes this ratio. Now this ratio is allowed to take on infinite values. But there's still one issue here. Let me move this over a bit. The one issue here is that m x plus b, that can still be negative, right? Like if, you know, I have a negative slope, if I have a negative b, if I have some negative x is in there, I don't know, but that can be, that's allowed to be negative. So how do we fix that? We do that by actually taking the log of the odds. Okay. So now I have the log of, you know, some probability divided by one minus the probability. And now that is on a range of negative infinity to infinity, which is good because the range of log should be negative infinity to infinity. Now, how do I solve for p the probability? Well, the first thing I can do is take, you know, I can remove the log by taking the net, the e to the, whatever is on both sides. So that gives me the probability over the one minus the probability is now equal to e to the m x plus b. Okay. So let's multiply that out. So the probability is equal to one minus probability e to the m x plus b. So p is equal to e to the m x plus b minus p times e to the m x plus b. And now we have, we can move like terms to one side. So if I do p, uh, so basically I'm moving this over. So I'm adding p. So now p one plus e","probability, negative, minus, log, infinity",Review the key concepts from this section and write a one-paragraph summary.
01:24:00,"to the m x plus b is equal to e to the m x plus b. And let me change this parenthesis, make it a little bigger. So now my probability can be e to the m x plus b divided by one plus e to the m x plus b. Okay. Well, let me just rewrite this really quickly. I want a numerator of one on top. Okay. So what I'm going to do is I'm going to multiply this by negative m x plus b. And then also the bottom by negative m x plus b. And I'm allowed to do that because this over this is one. So now my probability is equal to one over one plus e to the negative m x plus b. And now why do I rewrite it like that? It's because this is actually a form of a special function which is called the sigmoid function. And for the sigmoid function, it looks something like this. So s of x sigmoid, you know, that's some x is equal to one over one plus e to the negative x. So essentially what I just did up here is rewrite this in some sigmoid function where the x value is actually m x plus b. So maybe I'll change this to y just to make that a bit more clear. It doesn't matter what the variable name is. But this is our sigmoid function. And visually what our sigmoid function looks like is it goes from zero, so this here is zero to one. And it looks something","sigmoid function, sigmoid, function, negative, equal",Review the key concepts from this section and write a one-paragraph summary.
01:26:00,"like this curved s, which I didn't draw too well. Let me try that again. It's a hard to draw. Something if I can draw this right like that. Okay, so it goes in between zero and one. And you might notice that this form fits our shape up here. Oops, let's draw it sharper. But if it's our shape up there, a lot better, right? All right, so that is what we call logistic regression. We're basically trying to fit our data to the sigmoid function. And when we only have one data point, so if we only have one feature x, then that's what we call simple logistic regression. But then if we have x0, but then if we have x0, x1 all the way to xn, we call this multiple logistic regression. Because there are multiple features that we're considering when we're building our model. Logistic regression. So I'm going to put that here. And again, from sk learn, this linear model, we can import logistic regression, right? And just like how we did above, we can repeat all of this. So here instead of nb, I'm going to call this the log model or LG logistic regression. I'm going to change this to logistic regression. So I'm just going to use the default logistic regression.","logistic regression, regression, logistic, draw, call","Find a binary classification dataset (e.g., Titanic) and apply logistic regression using scikit-learn."
01:28:00,"But actually, if you look here, you see that you can use different penalties. So right now we're using an L2 penalty. But L2 is our quadratic formula. So that means that for outliers, it would really penalize that. For all these other things, you can toggle these different parameters. And you might get slightly different results. If I were building a production level logistic regression model, then I would want to go and I would want to figure out, what are the best parameters to pass into here based on my validation data. But for now, we'll just we'll just use this out of the box. So again, I'm going to fit the x-train and the y-train. And I'm just going to predict again. So I can just call this again. And instead of LG, nb, I'm going to use LG. So here this is decent. Precision 65%, recall 71, F168, or 82, total accuracy of 77. Okay, so it performs slightly better than night base, but it's still not as good as K and N. All right, so the last model for classification that I wanted to talk about is something called support vector machines or SVMs for short. So what exactly is an SVM model? I have two different features, x0 and x1 on the axes. And then I've told you if it's class 0 or class 1 based on the blue and the red labels. My goal is to find some sort of line between these two labels that best divides the data. All right, so this line is our SVM model. So I call it a line here because in","model, SVM model, penalties, line, SVM",Review the key concepts from this section and write a one-paragraph summary.
01:30:00,"2D, it's a line, but in 3D, it would be a plane. And then you can also have more and more dimensions. So the proper term is actually I want to find the hyperplane that best differentiates these two classes. Let's see a few examples. Okay, so first, between these three lines, let's say a, b, and c, which one is the best divider of the data? Which one has all the data on one side or the other? Or at least if it doesn't, which one divides it the most? Right? Which one has the most defined boundary between the two different groups? So this question should be pretty straightforward. It should be a, right? Because a has a clear distinct line between where, you know, everything on this side of a is one label. It's negative. And everything on this side of a is the other label. It's positive. So what if I have a, but then what if I had drawn my b like this? And my c maybe like this. Sorry, the labels are kind of close together. But now which one is the best? So I would argue that it's still a, right? And why is it still a? Because in these other two, look at how close this is to that to these points, right? So if I had some new point that I wanted to estimate, okay, say I didn't have a or b. So let's say we're","plane, side, line, data, label",Review the key concepts from this section and write a one-paragraph summary.
01:32:00,"just working with c. Let's say I have some new point that's right here. Or maybe a new point that's right there. Well, it seems like just logically looking at this. I mean, without the boundary, that would probably go under the positives, right? I mean, it's pretty close to that other positive. So one thing that we care about in SVNs is something known as the margin. Okay. So not only do we want to separate the two classes really well, we also care about the boundary in between where the points in those classes in our data set are and the line that we're drawing. So in a line like this, the closest values to this line might be like here. And I'm trying to draw these perpendicular. Right. And so this effectively, if I switch over to these dotted lines, if I can draw this right. So these effectively are what's known as the margins. Okay. So these both here, these are our margins in our SVNs. And our goal is to maximize those margins. So not only do we want the line that best separates the two different classes, we want the line that has the largest margin. And the data points that lie on the margin lines, the data. So basically, these are the data points that's helping us define our divider. These are what we call support","line, data, points, data points, margin",Review the key concepts from this section and write a one-paragraph summary.
01:34:00,"vectors. Hence the name support vector machines. Okay. So the issue with SVM sometimes is that they're not so robust to outliers. Right. So for example, if I had one outlier like this up here, that would totally change where I want my support vector to be, even though that might be my only outlier. Okay. So that's something to keep in mind as, you know, when you're working with SVMs, is it might not be the best model if there are outliers in your data set. Okay. So another example of SVMs might be, let's say that we have data like this, I'm just going to use a one dimensional data set for this example. Let's say we have a data set that looks like this. Well, our, you know, separators should be perpendicular to this line, but it should be somewhere along this line. So it could be anywhere like this. You might argue, okay, well, there's one here and then you could also just draw another one over here, right. And then maybe you can have two SVMs, but that's not really how SVMs work. But one thing that we can do is we can create some sort of projection. So I realized here that one thing I forgot to do was to label where zero was. So let's just say zero is here. Now what I'm going to do is I'm going to say, okay, I'm going to have X and then I'm going to have X, sorry, X zero and X one. So X zero is just going to be my original X, but I'm going to make X one equal to let's say X squared. So whatever is this squared, right. So now my negatives would be, you know, maybe somewhere here, here,","support vector, support vector machines, SVMs, data set, data",Review the key concepts from this section and write a one-paragraph summary.
01:36:00,"just pretend that it's somewhere up here. Right. And now my pluses might be something like that. And I'm going to run out of space over here. So I'm just going to draw these together. Use your imagination. But once I draw it like this, well, it's a lot easier to apply a boundary, right. Now our SVM could be maybe something like this, this. And now you see that we've divided our data set. Now it's separable where one class is this way. And the other class is that way. Okay. So that's known as SVMs. I do highly suggest that, you know, any of these models that we just mentioned, if you're interested in them, do go more in depth mathematically into them. Like how do we, how do we find this hyperplane? Right. I'm not going to go over that in this specific course because you're just learning what an SVM is. But it's a good idea to know, oh, okay, this is the technique behind finding, you know, what exactly are the, how do you define the hyperplane that we're going to use? So anyways, this transformation that we did down here, this is known as the kernel trick. So when we go from x to some coordinate x and then x squared, what we're doing is we are applying a kernel. So that's why it's called the kernel trick. So SVMs are actually really powerful. And you'll see that here. So from sklearn.svm, we are going to import SVC. And SVC is our support vector classifier. So with this, so with our SVM model, we are going to create","SVM, kernel, pretend, SVMs, kernel trick",Review the key concepts from this section and write a one-paragraph summary.
01:38:00,"SVC model. And we are going to, again, fit this to X-Train. I could have just copied and pasted this. I should have probably done that. Okay. Taking a bit longer. All right. Let's predict using our SVM model. And here, let's see if I can hover over this. Right. So again, you see a lot of these different parameters here that you can go back and change if you were creating a production level model. Okay. But in this specific case, we'll just use it out of the box again. So if I make predictions, you'll note that, wow, the accuracy actually jumps to 87% with the SVM. And even with class zero, there's nothing less than, you know, 0.8, which is great. And for class one, I mean, everything's at 0.9, which is higher than anything that we had seen to this point. So so far, we've gone over four different classification models. We've done SVMs, logistic regression, naive bays, and KNN. And these are just simple ways on how to implement them. Each of these, they have different, you know, they have different hyper parameters that you can go and you can toggle. And you can try to see if that helps later on or not. But for the most part, they perform, they give us around 70 to 80% accuracy. Okay. With SVM being the best. Now, let's see if we can actually beat that using a neural net. Now, the final type of model that I wanted to talk about is known as a neural net or neural network. And neural nets look something like this. So you have an input layer. This is where all your features would go. And they have all these arrows pointing to some sort","SVC model, SVC, neural, model, SVM",Review the key concepts from this section and write a one-paragraph summary.
01:40:00,"of hidden layer. And then all these arrows point to some sort of output layer. So what is all this mean? Each of these layers in here, this is something known as a neuron. Okay. So that's a neuron. In a neural net, these are all of our features that we're inputting into the neural net. So that might be X0, X1 all the way through XN. Right. And these are the features that we talked about. There, they might be, you know, the pregnancy, the BMI, the age, etc. Now, all of these get weighted by some value. So they are multiplied by some W number that applies to that one specific category, that one specific feature. So these could get multiplied. And the sum of all of these goes into that neuron. Okay. So basically I'm taking W0 times X0. And then I'm adding X1 times W1. And then I'm adding, you know, X2 times W2, etc., all the way to XN times WN. And that's getting input into the neuron. Now, I'm also adding this bias term, which just means, okay, I might want to shift this by a little bit. So I might add five, or I might add 0.1, or I might subtract 100. I don't know, but we're going to add this bias term. And the output of all these things. So the sum of this, this, this, and this go into something known as an activation function. Okay. And then after applying this activation function, we get an output. And this is what a neuron would look like. Now a whole network of them would look something like this. So I kind of lost over this activation function. What exactly is that? This is how a neural net looks like. If we have all our inputs here. And let's say all of these arrows represent some sort of addition.","hidden layer, neuron, times, activation function, function",Review the key concepts from this section and write a one-paragraph summary.
01:42:00,"Right. Then what's going on is we're just adding a bunch of times. Right. We're adding the sum sort of weight times these input layer a bunch of times. And then if we were to go back and factor that all out, then this entire neural net is just a linear combination of these input layers, which I don't know about you, but that just seems kind of useless. Right. Because we could literally just write that out in a formula. Why would we need to set up this entire neural network? We wouldn't. So the activation function is introduced. Right. So without an activation function, this just becomes a linear model. An activation function might look something like this. And as you can tell, these are not linear. And the reason why we introduced these is so that our entire model doesn't collapse on itself and become a linear model. So over here, this is something known as a sigmoid function. It runs between zero and one. Tansh runs between negative one all the way to one. And this is relu, which anything less than zero is zero. And then anything greater than zero is linear. So with these activation functions, every single output of a neuron is no longer just the linear combination of these. It's some sort of altered linear state, which means that the input into the next neuron is, you know, it doesn't, it doesn't collapse on itself. It doesn't become linear because we've introduced all these non-linearities. So this is a training set, the model, the loss. Right. And then we do this thing called training where we have to feed the loss back into the model and make certain adjustments the model to improve this predicted output. Let's talk a little bit about the training. What exactly goes on during that step?","linear, model, activation, function, activation function",Review the key concepts from this section and write a one-paragraph summary.
01:44:00,"Let's go back and take a look at our L2 loss function. This is what our L2 loss function looks like. It's a quadratic formula, right? Well, up here, the error is really, really, really, really large. And our goal is to get somewhere down here where the loss is decreased, right? Because that means that our predicted value is closer to our true value. So that means that we want to go this way. Okay. And thanks to a lot of properties of math, something that we can do is called gradient descent in order to follow this slope down this way. This quadratic is, it has different slopes with respect to some value. Okay. So the loss with respect to some weight W0 versus W1 versus Wn, they might all be different. Right. So some way that I kind of think about it is to what extent is this value contributing to our loss? And we can actually figure that out through some calculus, which we're not going to touch up on in this specific course. But if you want to learn more about neural nets, you should probably also learn some calculus and figure out what exactly back propagation is doing in order to actually calculate, you know, how much do we have to backstep by? So the thing is here, you might notice that this follows this curve at all of these different points. And the closer we get to the bottom, the smaller this step becomes. Now stick with me here. So my new value, this is what we call weight update. I'm going to take W0 and","loss function, loss, function, respect, quadratic",Review the key concepts from this section and write a one-paragraph summary.
01:46:00,"I'm going to set some new value for W0. And what I'm going to set for that is the old value of W0 plus some factor, which I'll just call alpha for now, times whatever this arrow is. So that's basically saying, okay, take our old W0, our old weight and just decrease it this way. So I guess increase it in this direction. Right. Like take a step in this direction. But this alpha here is telling us, okay, don't take a huge step. Right. Just in case we're wrong, take a small step. Take a small step in that direction. See if we get any closer. And for those of you who, you know, do want to look more into the mathematics of things, the reason why I use a plus here is because this here is the negative gradient. Right. If this were just the, if you were to use the actual gradient, this should be a minus. Now this alpha is something that we call the learning rate. Okay. And that adjusts how quickly we're taking steps. And that might, you know, tell our, that, that will ultimately control how long it takes for our neural net to converge. Or sometimes if you said it too high, it might even diverge. But with all of these weights. So here I have W0, W1, and then Wn, we make the same update to all of them after we calculate the loss, the gradient of the loss with respect to that weight. So that's how back propagation works. And that is everything that's going on here. After we calculate the loss, we're calculating gradients, making adjustments in the model. So we're setting all the, all the weights to something adjusted slightly. And then we're saying, okay, let's take the training set and run it through the model again and go through this loop all over again. So for machine learning, we already","set, step, direction, small step, alpha",Review the key concepts from this section and write a one-paragraph summary.
01:48:00,"have seen some libraries that we use, right? We've already seen SK learn. But when we start going into neural networks, this is kind of what we're trying to program. And it's not very fun to try to program this from scratch because not only will we probably have a lot of bugs, but also probably not going to be fast enough, right? When it be great, if there are just some full-time professionals that are dedicated to solving this problem, and they could literally just give us their code, that's already running really fast. Well, the answer is, yes, that exists. And that's why we use TensorFlow. So TensorFlow makes it really easy to define these models, but we also have enough control over what exactly we're feeding into this model. So for example, this line here is basically saying, okay, let's create a sequential neural net. So sequential is just, you know, what we've seen here, it just goes one layer to the next. And a dense layer means that all of them are interconnected. So here this is interconnected with all of these nodes and this one's all these. And then this one gets connected to all of the next ones and so on. So we're going to create 16 dense nodes with really activation functions. And then we're going to create another layer of 16 dense nodes with really activation. And then our output layer is going to be just one node. Okay. And that's how easy it is to define something in TensorFlow. So TensorFlow is an open source library that helps you develop and train your ML models. Let's implement this for a neural net. So we're using a neural net for classification now. So our neural net model, we are going to use TensorFlow. And I","neural, neural net, TensorFlow, net, layer",Review the key concepts from this section and write a one-paragraph summary.
01:50:00,"don't think I imported that up here. So we are going to import that down here. So I'm going to import TensorFlow as TF and enter cool. So my neural net model is going to be, I'm going to use this. So essentially this is saying layer all these things that I'm about to pass in. So yeah, layer them, linear stack of layers, layer them as a model. And what that means, nope, not that. So what that means is I can pass in some sort of layer. And I'm just going to use a dense layer. And let's say we have 32 units. Okay. I will also set the activation as re-loop. And at first we have to specify the input shape. So here we have 10 comma. All right. All right. So that's our first layer. Now our next layer, I'm just going to have another a dense layer of 32 units all using re-loop. And that's it. So for the final layer, this is just going to be my output layer. It's going to just be one node. And the activation is going to be sigmoid. So if you recall from our logistic regression, what happened there was when we had a sigmoid, it looked something like this. Right. So by creating a sigmoid activation on our last layer, we're essentially projecting our predictions to be zero or one, just like in logistic regression. And that's going to help us, you know, we can just round","layer, activation, sigmoid, import, imported",Review the key concepts from this section and write a one-paragraph summary.
01:52:00,"two zero or one and classify that way. So this is my neural net model. And I'm going to compile this. So intense workflow, we have to compile it. It's really cool because I can just literally pass in what type of optimizer I want and it'll do it. So here if I go to optimizers, I'm actually going to use atom. And you'll see that, you know, the learning rate is 0.001. So I'm just going to use that default, so 0.001. And my loss is going to be binary cross entropy. And the metrics that I'm also going to include on here, so it already will consider loss. But I'm also going to tack on accuracy. So we can actually see that in a plot later on. All right, so I'm going to run this. And one thing that I'm going to also do is I'm going to define these plot definitions. So I'm actually copying, pasting this. I got these from tensorflow. So if you go on to some tensorflow tutorial, they actually have these, this like defined. And that's exactly what I'm doing here. So I'm actually going to move the cell up, run that. So we're basically plotting the loss over all the different epochs. epochs means like training cycles. And we're going to plot the accuracy over all the epochs. All right. So we have our model. And now all that's left is let's train it. Okay. So I'm going to say history. So tensorflow is great because it keeps track of the history of the training, which is why we can go and plot it later on. Now I'm going to set that equal to this neural net model. And fit that with X trained Y trained. I'm going to make the number of epochs equal to let's","plot, epochs, model, loss, tensorflow",Review the key concepts from this section and write a one-paragraph summary.
01:54:00,"say just let's just use 100 for now. And the batch size, I'm going to set equal to let's say 32. All right. And the validation split. So what the validation split does, if it's down here somewhere. Okay. So yeah, this validation split is just the fraction of the training data to be used as validation data. So essentially every single epoch, what's going on is tensorflow saying leave certain, if this is 0.2, then leave 20% out. And we're going to test how the model performs on that 20% that we've left out. Okay. So it's basically like our validation data set. But tensorflow does it on our training data set during the training. So we have now a measure outside of just our validation data set to see, you know, what's going on. So validation split, I'm going to make that 0.2. And we can run this. So if I run that. All right. And I'm actually going to set verbose equal to 0, which means, okay, don't print anything. Because printing something for 100 epochs might get kind of annoying. So I'm just going to let it run, let it train. And then we'll see what happens. Cool. So it finished training. And now what I can do is because, you know, I've already defined these two functions. I can go ahead and I can plot the loss, oops, loss of that history. And I can also plot the accuracy throughout the training. So this is a little bit ish what we're looking for. We definitely are looking for a steadily decreasing loss and an increasing accuracy. So here we do see that,","validation, validation split, data, validation data, training",Review the key concepts from this section and write a one-paragraph summary.
01:56:00,"you know, our validation accuracy improves from around 0.77 or something all the way up to somewhere around 0. maybe 81. And our loss is decreasing. So this is good. It is expected that the validation loss and accuracy is performing worse than the training loss or accuracy. And that's because our model is training on that data. So it's adapting to that data, whereas the validation stuff is, you know, stuff that it hasn't seen yet. So, so that's why. So in machine learning, as we saw above, we could change a bunch of the parameters, right? Like I could change this to 64. So now it'd be a row of 64 nodes and then 32 and then one. So I can change some of these parameters. And a lot of machine learning is trying to find, hey, what do we set these hyper parameters to? So what I'm actually going to do is I'm going to rewrite this so that we can do something, what's known as a grid search. So we can search through an entire space of, hey, what happens if, you know, we have 64 nodes and 64 nodes or 16 nodes and 16 nodes and so on. And then on top of all that, we can, you know, we can change this learning rate, we can change how many epochs, we can change, you know, the batch size, all these things might affect our training. And just for kicks, I'm also going to add what's known as a drop-out layer.","nodes, change, validation accuracy improves, accuracy improves, accuracy",Review the key concepts from this section and write a one-paragraph summary.
